{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 \n",
    "\n",
    "Start looking in to a more realistic case -- evolving the full set of coefficients\n",
    "\n",
    "To have more latitude for improvement, and for more than bias being involved:\n",
    "\n",
    "\n",
    "ln -sf testin.2621.csv testin.csv \n",
    "\n",
    "\n",
    "in your directory. This station has very large rms and bias, and the errors show dependence on the GFS variables themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate python imports\n",
    "import sys\n",
    "import csv\n",
    "from math import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code being shown here is heavily towards the evolutionary side. \n",
    "\n",
    "The key physical science and mathematics are in the imported module evolution1.\n",
    "Keys being:\n",
    "how to translate the parameters in to a prediction\n",
    "how to score a prediction\n",
    "\n",
    "In this case, the full set of linear coefficients are being evolved, bias and coefficients for GFS -- t2m, td, thickness (1000-850 mb), rh, and wind speed.\n",
    "\n",
    "Though not shown yet, a good thing to do is to plot the predictions vs. their target. You can add that yourself, matplotlib is already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic1 from the github\n",
    "\n",
    "# Some global parameters:\n",
    "nobs = 579\n",
    "nparameters = 6\n",
    "\n",
    "npopulation = 10\n",
    "per_second = 60     # estimate of number of generations per second\n",
    "genmax = int(60*per_second)\n",
    "\n",
    "train_start = int(0)\n",
    "train_end   = int(364)\n",
    "np.random.seed(0)      # for reproducibility\n",
    "\n",
    "from scores import *\n",
    "from evolution2 import *\n",
    "\n",
    "######################## ######################## ########################\n",
    "# Now bring in the data for real work:\n",
    "matchup_set = []\n",
    "\n",
    "with open('testin.csv') as csvfile:\n",
    "    k = 0\n",
    "    sreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for line in sreader:\n",
    "        day     = float(line[0])\n",
    "        t2m_gfs = float(line[1])\n",
    "        td_gfs  = float(line[2])\n",
    "        thick_gfs = float(line[3])\n",
    "        rh_gfs  = float(line[4])\n",
    "        speed   = float(line[5])\n",
    "        obs_t2m = float(line[6])\n",
    "        obs_td  = float(line[7])\n",
    "        terr    = float(line[8])\n",
    "        tderr  = float(line[9])\n",
    "\n",
    "        #Note that obs_td, obs_t2m, tderr are being ignored. They can be\n",
    "        #       added to the list.\n",
    "        #  n.b.: note that it is terr that is used, not t2m itself.\n",
    "        #Model and observation are well-enough correlated that it is the increment\n",
    "        #which makes more sense to predict [Krasnopolsky,20NNN]\n",
    "        m = matchup((day,t2m_gfs,td_gfs,thick_gfs,rh_gfs,speed,terr))\n",
    "        matchup_set.append(m)\n",
    "        k += 1\n",
    "\n",
    "csvfile.close()\n",
    "######################## ######################## ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and seed the population\n",
    "\n",
    "Note the python structure used for initializing and adding to a list of things. Population and bests can be added to at will via the .append operation. We'll use this later (section 3) to collect all the parameter suites which are good in some respect (we'll decide what constitutes 'good').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic1 \n",
    "\n",
    "#You can change this and re-run to see the effects of changes to the metric used:\n",
    "#  Other options are:\n",
    "#  MEAN\n",
    "#  RM3 (root mean cubed error, will be related to skew)\n",
    "#  RM4 (root mean quartic error, will be related to kurtosis)\n",
    "#  MAE (Mean absolute error)\n",
    "#  NLOSS (count the number of times the corrected output is worse than the original); \n",
    "#      not currently working, will fall back to RMS\n",
    "#  VICKIE (MAE with a 3 C tolerance -- errors less than 3 C are ignored, otherwise, MAE)\n",
    "#    -- a representation of a real person's (my wife) interest in T2M accuracy.\n",
    "#The 'scores' file imported above contains the code and it is easy to add or modify\n",
    "\n",
    "measure = RMS\n",
    "\n",
    "#Initialize and seed the population\n",
    "population = []\n",
    "bests      = []       # Save all then-best versions\n",
    "for k in range (0,npopulation):\n",
    "    population.append(critter(nparameters))\n",
    "\n",
    "weights = np.zeros((nparameters))\n",
    "sdevs   = np.zeros((nparameters))\n",
    "bests.append(critter(nparameters))\n",
    "bests[0].init(weights, sdevs, 99.)\n",
    "nbests = 1\n",
    "\n",
    "#for reference, take the raw gfs output's score:\n",
    "population[0].init(weights, sdevs, 99.)\n",
    "score_gfs = population[0].skill(matchup_set, train_start, train_end, metric = measure)\n",
    "\n",
    "print(\"uncorrected score in training period: \",\n",
    "         population[0].skill(matchup_set, train_start, train_end, metric = measure) )\n",
    "print(\"uncorrected score in evaluation period: \",\n",
    "         population[0].skill(matchup_set, train_end+1, nobs, metric = measure), flush=True )\n",
    "population[0].show_fcst(matchup_set, train_start, train_end)\n",
    "\n",
    "population[0].weights[0] = 0.0\n",
    "\n",
    "print(\"\\n\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population and find our first best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the standard deviations for evolution ----------\n",
    "#For the bias\n",
    "sdevs[0] = 1.0\n",
    "#For linear terms\n",
    "for k in range (1,int(6)):\n",
    "    sdevs[k] = 1.0\n",
    "\n",
    "#For quadratic terms\n",
    "#for k in range (int(6), nparameters):\n",
    "#  sdevs[k] = 0.0125\n",
    "\n",
    "#Initialize the population itself now -------------------------\n",
    "for k in range (0,npopulation):\n",
    "  weights[0] = np.random.normal(0,sdevs[0])\n",
    "  for l in range (1, int(6) ):     #initialize only the linear part\n",
    "    weights[l] = np.random.normal(0,sdevs[l])\n",
    "  population[k].init(weights,sdevs, 99.)\n",
    "\n",
    "#recall that the matchup_set is holding the matchups\n",
    "#Find our first 'best' -- noting that we aren't saving raw gfs as an example\n",
    "smin = 9999.\n",
    "kbest = int(npopulation)\n",
    "for k in range (0,npopulation):\n",
    "    population[k].skill(matchup_set, train_start, train_end, measure)\n",
    "    if (population[k].score < smin):\n",
    "        kbest = k\n",
    "        smin = population[k].score\n",
    "\n",
    "#Start accumulating our best critters\n",
    "bests.append(critter(nparameters))\n",
    "bests[nbests].init(population[kbest].weights, population[kbest].sdevs, population[k].score)\n",
    "nbests += 1\n",
    "\n",
    "population[kbest].show()\n",
    "print(\"initial kbest, smin = \",kbest, smin, flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of evolution\n",
    "\n",
    "For this evolution, we are using only mutation -- as would happen with bacteria (haploid).\n",
    "\n",
    "As an analogy to diploids (plants, animals, people), one could also have 'crossover' mutations. Namely, to select two parents and take the first M genes from the first parent, and the remainder from the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## ######################## ########################\n",
    "#      Now carry out the (mutation-only) evolution\n",
    "#swap best in to all slots\n",
    "#then evolve a new raft of critters from that\n",
    "#evaluate them\n",
    "#repeat until limit of generations or happy\n",
    "\n",
    "for gen in range(0,genmax):\n",
    "    #print(\"generation \", gen, flush=True)\n",
    "\n",
    "    population[0].copy(population[kbest])\n",
    "    population[0].score = population[kbest].score\n",
    "    score_best = float(population[0].score)\n",
    "    smin = score_best\n",
    "    kbest = 0\n",
    "    for k in range (1, npopulation):\n",
    "        population[k].copy(population[0])\n",
    "        population[k].evolve()\n",
    "        population[k].skill(matchup_set, train_start, train_end, metric = RMS)\n",
    "        if (population[k].score < score_best):\n",
    "            kbest = k\n",
    "            smin = population[k].score\n",
    "            bests.append(critter(nparameters))\n",
    "            bests[nbests].init(population[kbest].weights, population[kbest].sdevs, population[kbest].score)\n",
    "            nbests += 1\n",
    "    if (kbest != 0):\n",
    "        if (score_gfs != 0):\n",
    "          print(\"new best \",gen, kbest, smin, score_best, smin/score_gfs, flush=True)\n",
    "        else:\n",
    "          print(\"new best \",gen, kbest, smin, score_best, flush=True)\n",
    "        population[kbest].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider what we found along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## ######################## ########################\n",
    "if (score_gfs != 0):\n",
    "  print(\"best score in training period \",gen, kbest, smin, score_best, smin/score_gfs, flush=True)\n",
    "else:\n",
    "  print(\"best score in training period \",gen, kbest, smin, score_best, flush=True)\n",
    "print(\"score in the untrained period: \",population[kbest].skill(matchup_set, train_end+1, nobs, measure))\n",
    "\n",
    "print(\"found \",nbests,\"new bests along the way\\n\")\n",
    "for k in range (0, nbests):\n",
    "  bests[k].show()\n",
    "  print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forecasts in the training period:\")\n",
    "population[0].show_fcst(matchup_set, train_start, train_end)\n",
    "\n",
    "#My standard run with RMS on 2621 finishes with \n",
    "# mean rms  0.30498389859902936 7.73783451221378\n",
    "\n",
    "#Uncorrected GFS: \n",
    "# mean rms  13.939368131868132 16.79408375517474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Untrained forecasts:\")\n",
    "population[0].show_fcst(matchup_set, train_end, nobs)\n",
    "\n",
    "# My standard run finishes with\n",
    "#mean rms  -2.1955708992208987 5.308750022304633\n",
    "\n",
    "# Uncorrected GFS: \n",
    "#mean rms  15.939368131868132 18.487799266091592\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "If you rerun cells after the initial np.seed(0), you'll get different results. Or, of course, you can change the seed and get different results.\n",
    "\n",
    "This particular station has such large errors, that almost anything (random guess) makes an improvement over the GFS raw output. That's why the first population gives a best that has only 56% the RMS of the raw output. A bit of further evolution reduced that to 44%.\n",
    "\n",
    "In saying 'a bit' of evolution, that is because we let the evolution run for only about 1 minute. The only reason for that limit is convenience in presentation. We could set that to something equivalent to an hour, or until the RMS is below 3.5 C (better than 90% of uncorrected stations). \n",
    "\n",
    "Note, too, the particular coding of this Python is not particularly efficient. In Fortran or C/C++, it is 10-100 times faster. Optimizing the usage of Python could also be much faster than this.\n",
    "\n",
    "Notice that there can be many generations (a few hundred, in the example) of no improvement in the best score. There are techniques for dealing with this. We'll look at some in section 3. Also, one can introduce a hill-climbing step every 100 steps of no improvement via evolution. More generally, to use standard optimizers after evolution has produced good candidates.\n",
    "\n",
    "### Experiment: \n",
    "Increase the maximum generation to, say, 100,000 and see what you get -- both in terms of the quality of result, and the set of coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter markdown boxes\n",
    "\n",
    "There are two types of boxes/cells, markdown and code. The heading type is being incorporated in markdown, examples below. After entering text in markdown, you can 'run' it (button above) and it will be formatted correctly.\n",
    "\n",
    "Handy way to make your own comments regarding code elements from a notebook.\n",
    "\n",
    "# This is a level 1 heading\n",
    "## 2 This is a level 2 heading\n",
    "### level 3\n",
    "\n",
    "Regular text in the markdown\n",
    "<i>italic text</i>\n",
    "<b>bold text</b>\n",
    "\n",
    "### Jupyter and Equations:\n",
    "For mathematics in the discussion and documents, we can use LaTeX markup with the modification that equations on their own line are noted by $$ before and after, rather than a \\begin \\end pair.\n",
    "\n",
    "Inline equation $ Volume = \\frac{4.}{3.}*\\pi*r^3 $, while offset equation is: $$ Volume = \\frac{4.}{3.}*\\pi*r^3 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Warming up with python and random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python -- import basic math and the numpy math modules\n",
    "#run this cell\n",
    "\n",
    "from math import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with random numbers.\n",
    "\n",
    "Evolutionary Strategies like gaussian distributions to the parameters, with lognormal distribution to the evolution of the standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with random numbers\n",
    "\n",
    "# Specifying the seed ensures that results are reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Give a mean and standard deviation for a normal distribution, and for a lognormal distribution\n",
    "# .. and then a number of elements to sample\n",
    "mean = 0\n",
    "sdev = 1.0\n",
    "npts = 3200\n",
    "x = np.random.normal(mean, sdev, npts)\n",
    "sx = np.random.lognormal(mean, sdev, npts)\n",
    "\n",
    "# Some demonstration output -- first element of the normal distribution, \n",
    "#    sample of the whole normal distribution and its max and min, \n",
    "#    sample of the lognormal distribution and its max and min.\n",
    "# lognormal min should be >= 0.0\n",
    "# Normal distribution mean should tend to zero as number of points increases\n",
    "# Try making several runs with different seeds and check this\n",
    "print(x[0])\n",
    "print(\"gaussian var N(0,1) \", x, x.max(), x.min(), x.mean() )\n",
    "print(\"lognormal var \",sx, sx.max(), sx.min() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In any of our model or algorithm development, we will be scoring the performance in \n",
    "#    some way, often in many ways.\n",
    "# Hence, there will always be some kind of scoring function, and it will return a scalar or maybe \n",
    "#    a vector/tuple/list of scores\n",
    "#\n",
    "# The score can be any function of the errors. RMSE is used here for demonstration purposes\n",
    "#   An exponential function of the prediction error is fine, for evolution (and will\n",
    "#   give very strange results -- try it)\n",
    "def score(delta, start, end, tolerance = 0):\n",
    "    tmp = delta[start:end]\n",
    "    tmp *= tmp\n",
    "    return sqrt(sum(tmp)/(end-start+1))\n",
    "    #tmp = np.exp(delta[start:end])\n",
    "    #return sum(tmp)/(end-start+1)\n",
    "    \n",
    "# Python note: the tolerance is an optional argument (and, at the moment, is not being used)\n",
    "#   If it is given in the function call, the value passed will be used, otherwise it \n",
    "#   defaults to zero.\n",
    "# While not used in this notebook, it will be used in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial of the scoring function\n",
    "\n",
    "This will execute the scoring function on the previous block's random numbers.\n",
    "\n",
    "As npts increases, the rmse will tend to the standard deviation. Try it.\n",
    "\n",
    "The start and end points recognize that we will be training on a subset of the data. Train on 0-N/2, score on N/2+1 to N, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(0)\n",
    "end = int(npts/2)\n",
    "print(x[0])\n",
    "print(start, end, score(x, start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check that the histograms look appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(x,range=(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(sx,range=(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring and displaying data\n",
    "\n",
    "The test input is for a station in the MOS data set (KCGZ, in AZ, if I counted right). It includes GFS values for T2M, Td, 850-1000mb thickness, rh, and wind speed. Then t2m-observed, td-observed, t2m-error, td-error.\n",
    "Once per day, for 579 days of data.\n",
    "\n",
    "Using csv module for parsing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nobs = 579\n",
    "day = np.zeros((nobs))\n",
    "t2m_gfs = np.zeros((nobs))\n",
    "td_gfs = np.zeros((nobs))\n",
    "thick_gfs = np.zeros((nobs))\n",
    "rh_gfs = np.zeros((nobs))\n",
    "speed = np.zeros((nobs))\n",
    "obs_t2m = np.zeros((nobs))\n",
    "obs_td = np.zeros((nobs))\n",
    "terr = np.zeros((nobs))\n",
    "tderr = np.zeros((nobs))\n",
    "#could also make up a class 'matchup', and have 579 of those\n",
    "\n",
    "with open('testin.csv') as csvfile:\n",
    "    k = 0\n",
    "    sreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for line in sreader:\n",
    "        day[k] = float(line[0])\n",
    "        t2m_gfs[k] = float(line[1])\n",
    "        td_gfs[k] = float(line[2])\n",
    "        thick_gfs[k] = float(line[3])\n",
    "        rh_gfs[k] = float(line[4])\n",
    "        speed[k] = float(line[5])\n",
    "        obs_t2m[k] = float(line[6])\n",
    "        obs_td[k] = float(line[7])\n",
    "        terr[k] = float(line[8])\n",
    "        tderr[k] = float(line[9])\n",
    "        k += 1\n",
    "csvfile.close()\n",
    "        \n",
    "print(day[0],t2m_gfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably you'd rather see a graphic, so here are t2m_gfs and observed:\n",
    "#matplotlib.use('Agg') #for batch mode\n",
    "#matplotlib.use('Qt5Agg')\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set(xlabel=\"Day number\", ylabel=\"C\")\n",
    "ax.plot(day,t2m_gfs,color='blue',label='GFS')\n",
    "ax.plot(day,obs_t2m,color='green',label='Obs')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kluge -- my desk isn't happy with the figure show\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set(xlabel=\"Day number\", ylabel=\"delta C\")\n",
    "ax.plot(day,t2m_gfs-obs_t2m)\n",
    "ax.plot(day,terr)\n",
    "ax.grid()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kluge again\n",
    "plt.close()\n",
    "\n",
    "#confirm that the difference between file's error and what we compute isn't just optically small:\n",
    "print((t2m_gfs-obs_t2m-terr).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#space for experimenting with plotting variables, say scatter plot of terr vs. the various gfs parameters:\n",
    "fig,ax = plt.subplots()\n",
    "ax.set(xlabel=\"t2m GFS\", ylabel=\"terr\")\n",
    "plt.scatter(t2m_gfs, terr)\n",
    "ax.grid()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "mean = terr.mean()\n",
    "rmse = sqrt(sum(terr*terr)/nobs)\n",
    "var  = sqrt(rmse*rmse-mean*mean)\n",
    "\n",
    "print(terr.mean(), terr.mean()*1.8) #C and F mean errors\n",
    "print(rmse, var, var/rmse)          #RMSE, variance\n",
    "\n",
    "r = np.correlate(terr,t2m_gfs) / sqrt(np.correlate(terr,terr)*np.correlate(t2m_gfs,t2m_gfs)) \n",
    "print(r,r*r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optical matter, it looks like there's no particular relation between how hot it is, according to the GFS, and how much the GFS is off by -- bias of almost 4 F. \n",
    "\n",
    "The bias being 2.15 and rms of 3.50 (rounded), suggests that simply applying a bias correction to the GFS can give substantial improvement on its own -- about 22% reduction in the rmse for removing the bias term.\n",
    "\n",
    "The correlation (np.correlate is computing covariance, so needs the normalization) being |0.54| suggests that our eyes are largely correct -- linear regression can explain about 29% of the variance. Better than the 22% of simple bias correction, but not by much (depending, of course, on one's needs).\n",
    "\n",
    "Of course we have more variables available, and no particular reason to believe that linear functions are the only, much less necessarily the best, ones to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for the Evolutionary Strategy\n",
    "\n",
    "It is the ambiguity about which the best variables are, what the best functions of them might be, and how best to combine them which encourages looking to evolutionary strategies -- to find improved mappings between the 5 GFS variables at hand and the one item we're trying to predict (or to correct). The principles generalize to many more input variables and many more outputs. Also, one may converge the Evolutionary Strategy towards a Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchup:\n",
    "    #values is a set (tuple or np.ndarray) of parameter values in the matchup\n",
    "    def __init__(self, values): \n",
    "        self.values = values\n",
    "    \n",
    "    #display element by element the values of the matchup\n",
    "    def show(self):\n",
    "        n = len(self.values)\n",
    "        #Note that in python for loop ranges, the iteration is over [0,n)\n",
    "        for k in range(0,n):\n",
    "            print(k,self.values[k])\n",
    "    \n",
    "    #extract the k-th parameter from the values tuple\n",
    "    def __getitem__(self,k):\n",
    "        return(self.values[k])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = matchup((0,1,2,3))\n",
    "x.show()\n",
    "\n",
    "#making a toy variable to hold a vector of matchups.\n",
    "#note syntax for adding new elements.\n",
    "y = []\n",
    "y += [x]\n",
    "y += [x]\n",
    "y += [x]\n",
    "print('number of elements in y: ',len(y))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now bring in the data for real work:\n",
    "z = []\n",
    "\n",
    "  \n",
    "with open('testin.csv') as csvfile:\n",
    "    k = 0\n",
    "    sreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for line in sreader:\n",
    "        day = float(line[0])\n",
    "        t2m_gfs = float(line[1])\n",
    "        td_gfs = float(line[2])\n",
    "        thick_gfs = float(line[3])\n",
    "        rh_gfs = float(line[4])\n",
    "        speed = float(line[5])\n",
    "        obs_t2m= float(line[6])\n",
    "        obs_td = float(line[7])\n",
    "        terr = float(line[8])\n",
    "        tderr = float(line[9])\n",
    "        \n",
    "        #Note that obs_td, obs_t2m, tderr are being ignored. They can be added to the list.\n",
    "        #  n.b.: note that it is terr that is used as the variable to predict, not t2m itself. \n",
    "        #Model and observation are well-enough correlated that it is the increment \n",
    "        #which makes more sense to predict [Krasnopolsky,20NNN]\n",
    "        m = matchup((day,t2m_gfs,td_gfs,thick_gfs,rh_gfs,speed,terr))\n",
    "        z.append(m)\n",
    "        k += 1\n",
    "    \n",
    "csvfile.close()\n",
    "\n",
    "#display the last-added element\n",
    "print(m)\n",
    "m.show()\n",
    "\n",
    "#display it from the matchup vector\n",
    "print(\"\\n z len = \",len(z),'\\n')\n",
    "z[len(z)-1].show()\n",
    "print(\"\\n\")\n",
    "\n",
    "#print t2m_gfs specifically -- two different ways of referencing it. \n",
    "#The first emphasizes that z[0] is the entity, and we want the element numbered '1' from it. \n",
    "#  Remembering that indexing goes from 0 in Python.\n",
    "print((z[0])[1], z[0][1])\n",
    "print((z[nobs-1])[1], z[nobs-1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now z is holding our matchup set of variables. \n",
    "\n",
    "We now need a function whose arguments will be a matchup and a set of constants\n",
    "* make a prediction about what terr should be\n",
    "* return how incorrect it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction from variables in the matchup x, using constants in the list y\n",
    "#First prediction method:\n",
    "def predict1(x,y):\n",
    "    nx = len(x.values)\n",
    "    #will ignore matchup[0] (day of observation) and matchup[6], the error term, \n",
    "    #  in making prediction\n",
    "    #Starting point: simple multi-linear regression, a bias term plus \n",
    "    #  weights(y[k]) times the predictors\n",
    "    pred = y[0]\n",
    "    for k in range(1,nx-1):\n",
    "        pred += x.values[k]*y[k]\n",
    "    #print(pred, x.values[nx-1], x[nx-1])\n",
    "    return (x.values[nx-1]+pred)\n",
    "\n",
    "#take a set of matchups and evaluate (ultimately, to score) the predictions from predict1\n",
    "#  note that we're now applying a start and end time -- the training period\n",
    "def evaluator1(z, start, end, y):\n",
    "    deltas = np.zeros((end-start+1))\n",
    "    for k in range (start, end):\n",
    "        deltas[k-start] = predict1(z[k],y)\n",
    "    #print(deltas)\n",
    "    return score(deltas,0,len(deltas))\n",
    "\n",
    "#n.b. would be desirable to have a general evaluator that takes the \n",
    "#    prediction function as an argument as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simplest, let's try the prediction being just the bias term, 2.15 C, we found above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros((6))\n",
    "\n",
    "weights[0] = 2.15\n",
    "print(evaluator1(z,0,364,weights)) # score on the first year of data, where we'll do the evolving\n",
    "print(evaluator1(z,365,nobs-1,weights)) #now for the remainder of the span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For comparison, try without any corrections: -- bring this forward\n",
    "weights[0] = 0.0\n",
    "print(evaluator1(z,0,364,weights)) \n",
    "print(evaluator1(z,365,nobs-1,weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### towards the evolutionary strategy\n",
    "\n",
    "Discussion of what exactly is evolutionary will wait for next section. For now, let us continue building the framework. \n",
    "\n",
    "There will be a population. Each member of the population will have a set of weights (coefficients/parameter values/... -- numbers that the evaluator will make use of to make a prediction), a set of standard deviations (to specify how the weights may evolve), and a score. It is up to the evaluator, already written, to find the score, also already written. So it is the evolver that is currently needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros((6))\n",
    "weights[0] = 0.0\n",
    "\n",
    "#Notice, Warning, only the first element has a nonzero standard deviation,\n",
    "#  so only the first element will be nonzero when we print out the weights\n",
    "sdevs = np.zeros((6))\n",
    "sdevs[0] = 1.0\n",
    "\n",
    "\n",
    "def evolve(weights, sdevs):\n",
    "    #sdevs[0]   = np.random.lognormal(sdevs[0],0.25)\n",
    "    for k in range (0,len(weights)):\n",
    "        weights[k] = np.random.normal(weights[k],sdevs[k])\n",
    "\n",
    "evolve(weights,sdevs)\n",
    "print(weights)\n",
    "print(sdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create a class to hold one of the critters (things we're going to evolve). \n",
    "Biologically speaking, these are bacteria. They evolve only by descent with mutation from a single parent, unlike diploid things, like humans, which can get genes/values from two parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critter:\n",
    "    score = float(99.0)\n",
    "    \n",
    "    def __init__(self, nparm): \n",
    "        self.weights = np.zeros((nparm))\n",
    "        self.sdevs = np.zeros((nparm))\n",
    "    \n",
    "    def init(self, weights, sdevs):\n",
    "        for k in range(0,len(weights)):\n",
    "          self.weights[k] = weights[k]\n",
    "          self.sdevs[k]   = sdevs[k] \n",
    "            \n",
    "    def copy(self, x):\n",
    "        for k in range(0,len(x.weights)):\n",
    "          self.weights[k] = x.weights[k]\n",
    "          self.sdevs[k]   = x.sdevs[k]\n",
    "        #self.show()\n",
    "\n",
    "    #display element by element the weights and sdevs\n",
    "    def show(self):\n",
    "        n = len(self.weights)\n",
    "#        for k in range(0,n):\n",
    "        for k in range(0,1):\n",
    "            print(k,self.weights[k], self.sdevs[k])\n",
    "            \n",
    "    def evolve(self):\n",
    "        evolve(self.weights, self.sdevs)\n",
    "    \n",
    "    def skill(self, matchups, start, end):\n",
    "        self.score = evaluator1(matchups, start, end, self.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial population for evolution\n",
    "From evolutionary biology, the Hardy-Weinberg equilibrium, the population needs to be small enough to evolve, large enough to survive. We will cheat this some by always keeping the previous best in our next generation.\n",
    "\n",
    "### initially, just try to re-evolve the constant bias correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npopulation = 10\n",
    "population = []\n",
    "\n",
    "nparameters = 6\n",
    "\n",
    "for k in range (0,npopulation):\n",
    "    population.append(critter(nparameters))\n",
    "    \n",
    "population[npopulation-1].show()\n",
    "\n",
    "sdevs[0] = 1.0\n",
    "for k in range (0,npopulation):\n",
    "    weights[0] = np.random.normal(0,1)\n",
    "    population[k].init(weights,sdevs)\n",
    "    print(k, population[k].weights[0], population[0].weights[0] )\n",
    "\n",
    "population[0].show()\n",
    "population[1].show()\n",
    "\n",
    "#How to copy element\n",
    "population[1].copy(population[0])\n",
    "population[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall that the poorly-named 'z' is holding the matchups\n",
    "smin = 9999.\n",
    "kbest = int(99)\n",
    "\n",
    "for k in range (0,npopulation):\n",
    "    population[k].skill(z, 0, 364)\n",
    "    if (population[k].score < smin):\n",
    "        kbest = k\n",
    "        smin = population[k].score \n",
    "    \n",
    "print(\"k, smin = \",kbest, smin)\n",
    "population[kbest].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap best in to all slots\n",
    "#then evolve a new population of critters from that\n",
    "#evaluate them\n",
    "#repeat until limit of generations or you are satisfied\n",
    "generation_max = int(300)\n",
    "population[kbest].show()\n",
    "\n",
    "#Not necessary to redefine, but gives somewhat better results for \n",
    "population[0].sdevs[0] = 0.25\n",
    "\n",
    "#So many simplifications have been made so already that little difference between \n",
    "#  (population,generation limit) being (10,300) vs. (3000,1)\n",
    "#Our demonstrations will soon become more complex and the difference will be very large\n",
    "\n",
    "for gen in range(0,generation_max):\n",
    "    population[0].copy(population[kbest])\n",
    "    population[0].score = population[kbest].score\n",
    "    score_best = float(population[0].score)\n",
    "    smin = score_best\n",
    "    kbest = 0\n",
    "    #Try npop times, then evolve from the best of these\n",
    "    for k in range (1, npopulation):\n",
    "        population[k].copy(population[0])\n",
    "        population[k].evolve()\n",
    "        population[k].skill(z,0,364)\n",
    "        #Sense of score is that smaller is better\n",
    "        if (population[k].score < score_best):\n",
    "            kbest = k\n",
    "            smin = population[k].score\n",
    "    if (kbest != 0):\n",
    "        print(\"new best \",gen, kbest, smin, score_best)\n",
    "        population[kbest].show()\n",
    "        \n",
    "population[kbest].show()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(evaluator1(z, 365,nobs,population[kbest].weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
